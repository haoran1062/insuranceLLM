# -*- coding: utf-8 -*-
"""
@author:XuMing(xuming624@qq.com)
@description: 
"""
import os

os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"
from transformers import LlamaTokenizer
from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model
import sentencepiece as spm
import argparse


def is_chinese(uchar):
    """åˆ¤æ–­ä¸€ä¸ªunicodeæ˜¯å¦æ˜¯æ±‰å­—"""
    return '\u4e00' <= uchar <= '\u9fa5'


def is_chinese_string(string):
    """åˆ¤æ–­æ˜¯å¦å…¨ä¸ºæ±‰å­—"""
    return all(is_chinese(c) for c in string)


def load_baichuan_vocab(vocab_file):
    words = set()
    with open(vocab_file, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                words.add(line.strip().split()[0])
    return words


def load_jieba_vocab(jieba_vocab_file):
    # Read jieba vocab and sort by freq
    with open(jieba_vocab_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
        word_freqs = [line.strip().split() for line in lines]
        word_freqs.sort(key=lambda x: int(x[1]), reverse=True)
    return word_freqs


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--base_tokenizer_dir', default=None, type=str, required=True)
    parser.add_argument('--domain_sp_model_file', default='./domain_sp.model', type=str)
    parser.add_argument('--baichuan_vocab_file', default="data/vocab/baichuan_vocab.txt", type=str)
    parser.add_argument('--add_jieba', action='store_true', help='Whether to add jieba vocab.')
    parser.add_argument('--jieba_word_freq_file', default='data/vocab/word_freq.txt', type=str)
    parser.add_argument('--jieba_word_size', default=20000, type=int)

    args = parser.parse_args()
    print(args)

    # load
    llama_tokenizer = LlamaTokenizer.from_pretrained(args.base_tokenizer_dir)
    chinese_sp_model = spm.SentencePieceProcessor()
    chinese_sp_model.Load(args.domain_sp_model_file)

    llama_spm = sp_pb2_model.ModelProto()
    llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())
    chinese_spm = sp_pb2_model.ModelProto()
    chinese_spm.ParseFromString(chinese_sp_model.serialized_model_proto())

    # print number of tokens
    print(len(llama_tokenizer), len(chinese_sp_model))
    print(llama_tokenizer.all_special_tokens)
    print(llama_tokenizer.all_special_ids)
    print(llama_tokenizer.special_tokens_map)

    # Add Chinese tokens to LLaMA tokenizer
    llama_spm_tokens_set = set(p.piece for p in llama_spm.pieces)

    print(len(llama_spm_tokens_set))
    print(f"Before:{len(llama_spm_tokens_set)}")
    added_set = set()
    for p in chinese_spm.pieces:
        piece = p.piece
        if piece not in llama_spm_tokens_set:
            # print('picec', piece)
            new_p = sp_pb2_model.ModelProto().SentencePiece()
            new_p.piece = piece
            new_p.score = 0
            llama_spm.pieces.append(new_p)
            added_set.add(piece)
    print(f"[add domain tokens]New model pieces: {len(llama_spm.pieces)}")

    vocab = load_baichuan_vocab(args.baichuan_vocab_file)
    print('baichuan vocab len:', len(vocab))
    baichuan_vocab_set = set([i for i in vocab if is_chinese_string(i)])
    print('baichuan chinese vocab size:', len(baichuan_vocab_set))
    print('baichuan vocab head:', list(baichuan_vocab_set)[:10])
    for p in baichuan_vocab_set:
        piece = p
        if piece not in llama_spm_tokens_set and piece not in added_set:
            # print('baichuan picec', piece)
            new_p = sp_pb2_model.ModelProto().SentencePiece()
            new_p.piece = piece
            new_p.score = 0
            llama_spm.pieces.append(new_p)
            added_set.add(piece)
    print(f"[add baichuan tokens]New model pieces: {len(llama_spm.pieces)}")

    if args.add_jieba:
        word_freqs = load_jieba_vocab(args.jieba_word_freq_file)
        top_words = word_freqs[:args.jieba_word_size]
        print('jieba top10 freq words:', top_words[:10])
        jieba_vocab_set = set([i[0] for i in top_words if i])
        print('jieba_vocab_set size:', len(jieba_vocab_set))
        print('jieba_vocab head:', list(jieba_vocab_set)[:3])
        for p in jieba_vocab_set:
            piece = p
            if piece not in llama_spm_tokens_set and piece not in added_set:
                # print('jieba picec', piece)
                new_p = sp_pb2_model.ModelProto().SentencePiece()
                new_p.piece = piece
                new_p.score = 0
                llama_spm.pieces.append(new_p)
        print(f"[add jieba tokens]New model pieces: {len(llama_spm.pieces)}")

    # Save
    output_sp_dir = 'merged_tokenizer_sp'
    output_hf_dir = 'merged_tokenizer_hf'  # the path to save Chinese-LLaMA tokenizer
    os.makedirs(output_sp_dir, exist_ok=True)
    with open(output_sp_dir + '/chinese_llama.model', 'wb') as f:
        f.write(llama_spm.SerializeToString())
    tokenizer = LlamaTokenizer(vocab_file=output_sp_dir + '/chinese_llama.model')

    tokenizer.save_pretrained(output_hf_dir)
    print(f"Chinese-LLaMA tokenizer has been saved to {output_hf_dir}")

    # Test
    llama_tokenizer = LlamaTokenizer.from_pretrained(args.base_tokenizer_dir)
    chinese_llama_tokenizer = LlamaTokenizer.from_pretrained(output_hf_dir)
    print(chinese_llama_tokenizer.all_special_tokens)
    print(chinese_llama_tokenizer.all_special_ids)
    print(chinese_llama_tokenizer.special_tokens_map)
    print('old len:', len(llama_tokenizer), ' new len:', len(chinese_llama_tokenizer))
    text = '''this is a test, hello world. thisisatesthelloworld, 
æ…•å®¹å¤æ¥åˆ°æ²³è¾¹ï¼Œå§‘è‹æ…•å®¹æ°åœ¨å¤–é¢ä¸¢äº†äººã€‚
1å·åº—ä¸€å‘¨å²äº†ï¼Œæˆ‘ä»¬ä¸€å¤è„‘å„¿ä¹°äº†10æ–¤é›¶é£Ÿã€‚
å·´å¡ç½—é‚£è¶³çƒä¿±ä¹éƒ¨ç®€ç§°å·´è¨ï¼ˆBarÃ§aï¼‰ï¼Œæ˜¯ä¸€å®¶ä½äºè¥¿ç­ç‰™åŠ æ³°ç½—å°¼äºšå·´å¡ç½—é‚£çš„è¶³çƒä¿±ä¹éƒ¨ï¼Œäº1899å¹´ç”±ç‘å£«ä¼ä¸šå®¶èƒ¡å®‰Â·ç”˜ä¼¯æ‰€åˆ›ç«‹ï¼Œä¸–ç•Œçƒå›é¡¶çº§è¶³çƒä¿±ä¹éƒ¨ä¹‹ä¸€ã€‚ä¿±ä¹éƒ¨ä¸»åœºå¯å®¹çº³æ¥è¿‘åä¸‡åè§‚ä¼—ï¼Œæ˜¯å…¨æ¬§æ´²æœ€å¤§åŠä¸–ç•Œç¬¬äºŒå¤§çš„è¶³çƒåœºã€‚
ç™½æ—¥ä¾å±±å°½ï¼Œé»„æ²³å…¥æµ·æµã€‚æ¬²ç©·åƒé‡Œç›®ï¼Œæ›´ä¸Šä¸€å±‚æ¥¼ã€‚æ‚¨å¥½ï¼Œè¯·é—®æ‚¨æ˜¯ä¸ºè‡ªå·±/å®¶äººäº†è§£ä¿é™©å‘¢ï¼Ÿæ˜¯éœ€è¦é…ç½®åŒ»ç–—é™©/é‡ç–¾é™©/å¯¿é™©/æ„å¤–é™©ï¼Œè¿˜æ˜¯ç†è´¢é™©å‘¢ï¼Ÿå¯ä»¥è·Ÿæˆ‘è¯´ä¸‹æ‚¨çš„éœ€æ±‚å“ˆ[ç«ç‘°] å®¢æˆ·: è¿™ä¸èƒ½æŠ¥å— é”€å”®: æŠ¥é”€çš„æ˜¯åŒ»ä¿ç›®å½•å†…çš„é¡¹ç›® é”€å”®: é—¨è¯Šé™©æœ‰2ç§ï¼šå°ç¥å…½è·Ÿå°å°ç¥å…½éƒ½æ˜¯ç»™å°å­©æŠ¥é”€çœ‹é—¨è¯Šçš„è´¹ç”¨çš„ä¿é™©ï¼Œ0å…èµ”é¢ï¼ŒæŠ¥é”€æ¯”ä¾‹ï¼šç»è¿‡ç¤¾ä¿æŠ¥é”€åçš„æŒ‰50%æŠ¥ï¼Œæœªç»è¿‡ç¤¾ä¿æŠ¥é”€çš„æŒ‰40%æŠ¥ã€‚å°ç¥å…½æ˜¯ä¸€å¹´æœŸçš„ä»·æ ¼ä¸º399å…ƒï¼Œå°å°ç¥å…½æ˜¯åŠå¹´æœŸçš„ä»·æ ¼ä¸º178å…ƒã€‚å°ç¥å…½ï¼šå•æ¬¡é™é¢500å…ƒï¼Œä¸€å¹´ç´¯è®¡æŠ¥é”€é¢åº¦2000å…ƒï¼›å°å°ç¥å…½ï¼šå•æ¬¡é™é¢300å…ƒï¼Œä¸€å¹´ç´¯è®¡æŠ¥é”€é¢åº¦1000å…ƒã€‚ä¸¤ä¸ªçš„ä¿éšœæ˜¯å·®ä¸å¤šçš„ï¼Œå°±çœ‹ä½ æƒ³è¦ç»™å°å­©å­ä¿åŠå¹´çš„è¿˜æ˜¯ä¸€å¹´çš„ï¼Ÿæˆ‘æ›´æ¨è é€‰ä¸€å¹´çš„ï¼Œå› ä¸ºä¸€å¹´æœŸçš„å¯ä»¥å¾ˆå¥½çš„è¦†ç›–æµæ„Ÿé«˜å‘çš„æ˜¥å­£è·Ÿç§‹å­£ï¼Œä»¥åŠå®¹æ˜“ç€å‡‰æ„Ÿå†’çš„å†¬å¤©ã€‚ å®¢æˆ·: ç´¯è®¡æŠ¥é”€2000æˆ–1000 å®¢æˆ·: æœ‰æ²¡æœ‰å¸¦ä½é™¢æŠ¥é”€ä¸€èµ·çš„é‚£ç§ é”€å”®: è¿™æ¬¾æ˜¯åªèƒ½æŠ¥é—¨è¯Šçš„ï¼Œä½é™¢çš„ä¸èƒ½æŠ¥å“ˆ å®¢æˆ·: å…¶ä»–äº§å“ å®¢æˆ·: å‘æ¥çœ‹çœ‹ é”€å”®: è¿™æ¬¾æ˜¯åŒ…å«äº†ç–¾ç—…ä½é™¢ï¼Œä¾‹å¦‚å­©å­å› ä¸ºè‚ºç‚ä½é™¢æ²»ç–—ï¼Œè¿™ç§è´¹ç”¨æ¯”è¾ƒä½çš„ï¼Œå‡ åƒå…ƒçš„ä½é™¢è´¹ç”¨ï¼Œå¯ä»¥ç”¨è¿™æ¬¾æ„å¤–é™©å»æŠ¥é”€ï¼ŒåŒ»ä¿æŠ¥å®Œä¹‹åè‡ªå·±åªéœ€è¦æ‰¿æ‹…300é™¢è´¹ç”¨ï¼Œå‰©ä½™éƒ¨åˆ†ä¿é™©å…¬å¸ç»™æŠ¥é”€æ‰ï¼Œè¿˜æ˜¯å¾ˆå®ç”¨çš„ã€‚ å®¢æˆ·: å¯¹äº† å°ç¥å…½ é—¨æ€¥è¯Š æ˜¯åªæŠ¥é”€æŒ‚å·è´¹å— å®¢æˆ·: å†™çš„ä¸æ˜¯å¾ˆæ¸…æ¥š å®¢æˆ·: æ¯”å¦‚è¯´ æŒ‚å·ä¹‹ååšçš„ä¸€äº›æ£€æŸ¥ å¯ä»¥æŠ¥é”€å— å®¢æˆ·: ç»™ä¸ªè´­ä¹°é“¾æ¥ å®¢æˆ·: æˆ‘å®å®ç°åœ¨æ‰14å¤©ï¼Œä»–æ˜¯ä¸æ˜¯è¦ç­‰åˆ°30å¤©åæ‰ç”Ÿæ•ˆ å®¢æˆ·: å“¦ è¿™æ · é”€å”®: æ˜¯çš„å“ˆ å®¢æˆ·: é‚£æ„å¤–é™©çš„è¯ å¯¹äºç–¾ç—…æœ‰ä»€ä¹ˆé™åˆ¶å— å®¢æˆ·: è¿˜æ˜¯åªè¦ä½é™¢ å°±å¯ä»¥æŠ¥é”€ é”€å”®: è¿™æ˜¯ä¸€æ¡å¼•ç”¨/å›å¤æ¶ˆæ¯ï¼š â€œå¥”æ³¢å„¿çï¼š é‚£æ„å¤–é™©çš„è¯ å¯¹äºç–¾ç—…æœ‰ä»€ä¹ˆé™åˆ¶å—â€
è¿™ä¸ªæ²¡æœ‰é™åˆ¶çš„ é”€å”®: åªè¦æ˜¯å›½å†…äºŒçº§ä»¥ä¸Šå…¬ç«‹åŒ»é™¢ä½é™¢ é”€å”®: æ˜¯çš„ å®¢æˆ·: ä¸‰ä¸ªä»·æ ¼æ˜¯æ€ä¹ˆåŒºåˆ†çš„ é”€å”®: å¹´é¾„åŒºåˆ†çš„ å®¢æˆ·: é‚£æˆ‘å®¶å´½æ˜¯å¤šå°‘é’± é”€å”®: 349å…ƒ/å¹´ é”€å”®: å¹´é¾„è¶Šå¤§æœ€ä¼˜æƒ çš„ å®¢æˆ·: ok å®¢æˆ·: è´­ä¹°é“¾æ¥å‘ç»™æˆ‘ é”€å”®: ä¹Ÿæ˜¯éœ€è¦ç­‰30å¤©æ»¡æœˆæ‰èƒ½æŠ•å‘¢äº²äº² é”€å”®: ç›®å‰è¿˜æ²¡æœ‰é’ˆå¯¹æ–°ç”Ÿå„¿èƒ½ä¹°çš„äº§å“ å®¢æˆ·: åˆ°æ—¶å€™è”ç³»æˆ‘å§ é”€å”®: éƒ½æ˜¯éœ€è¦æ»¡æœˆä¹‹åæ‰èƒ½æŠ•ä¿ é”€å”®: å¥½çš„ï¼Œæ²¡é—®é¢˜ğŸ‘Œ é”€å”®: æˆ‘è®°ç€è¿™äº‹ é”€å”®: å­©å­è¿˜æœ‰16å¤©æ‰æ»¡æœˆå¯¹å§ï¼Ÿ é”€å”®: æœ¬æœˆ19å·å¯¹å—ï¼Ÿ å®¢æˆ·: æ˜¯çš„ å®¢æˆ·: 20å·å‡ºç”Ÿçš„ é”€å”®: å¥½çš„ é”€å”®: æˆ‘å¤‡æ³¨ä¸€ä¸‹ å®¢æˆ·: å—¯ å®¢æˆ·: ä½ ä»¬å®¶å¯¿é™©å‘æˆ‘çœ‹ä¸‹ é”€å”®: æˆ‘ç»™æ‚¨çœ‹çœ‹ é”€å”®: å¾ˆå¤šäº§å“åœ¨7.31ä¸‹äº† é”€å”®: æ‰€ä»¥ç›®å‰æˆ‘çœ‹çœ‹æœ‰æ²¡æœ‰ä¸Šæ–°çš„ å®¢æˆ·: å¥½ é”€å”®: [ç«ç‘°]'''
    print("Test text:\n", text)
    print(f"Tokenized by LLaMA tokenizer:{llama_tokenizer.tokenize(text)}")
    print(f"Tokenized by Chinese-LLaMA tokenizer:{chinese_llama_tokenizer.tokenize(text)}")


if __name__ == '__main__':
    main()
